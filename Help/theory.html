<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>NeuralNetCreator - Nápověda</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>
	<div class="top">
		<h1>Nápověda programu Neural network Creator</h1>
		<div class="line2"></div>
		<div class="menu">
			<a href="theory.html">Teorie neuronových sítí</a>
			<a href="examples.html">Ukázkové příklady</a>
			<a href="guide.html">Ovládání programu</a>
			<a href="index.html">Obsah nápovědy</a>
		</div>
		<div class="line"></div>
	</div>
	<div class="main">
		<h2>Teorie neuronových sítí</h2>
		<p>Tato sekce nápovědy vysvětluje základy teorie neuronových sítí. První část sekce se zabývá popisem neuronu, jakožto prvku, ze kterého je složena celá neuronová síť. Vícevrstvá neuronová síť včetně několika doporučení k volbě topologie je popsána v druhé části sekce. Poslední část sekce je věnována algoritmu backpropagation.</p>
		
		
		<h3 id="sec1">Neuron</h3>
		
		<p>Neuron (viz obrázek 1) je základním stavebním kamenem každé neuronové sítě. Každý neuron má vstupy x<sub>1</sub> až x<sub>n</sub> opatřených vahami w<sub>1</sub> až w<sub>n</sub>. Váhy vstupů mohou být i záporné, čímž je vyjádřen jejich inhibiční charakter. Vážená suma vstupních hodnot představuje vnitřní potenciál neuronu, který lze vypočítat podle následujícího vztahu. [3]</p>

		<div class="image">
			<img src="theory/eq1.png"/>
		</div>

		<p>Pokud k neuronu přidáme vedlejší vstup o stálé hodnotě 1, můžeme bias včlenit do vztahu jako hodnotu váhy tohoto vstupu. Konečný výstup neuronu potom dostaneme dosazením vnitřního potenciálu neuronu do jeho přenosové funkce. [3]</p>
		
		<div class="image">
			<img src="theory/neuron.png"/>
			<div class="caption">Obr. 1: Neuron</div>
		</div>
		
		
		<h3 id="sec2">Přenosová funkce neuronu</h3>
		
		<p>Nejdůležitějšími přenosovými (aktivačními) funkcemi (viz obrázek 2) v neuronových sítích typu backpropagation jsou binární a unipolární sigmoidní funkce. I když se většinou předpokládá použití stejné přenosové funkce pro celou neuronovou síť nebo alespoň její jednotlivé vrstvy, není to pravidlem a každý neuron teoreticky může mít jinou přenosovou funkci. [1]</p>

		<p>Uvedené sigmoidní aktivační funkce jsou užitečné u neuronových sítí typu backpropagation zejména proto, že je lze snadno zderivovat a dosazení do již zderivované funkce snižuje výpočetní náročnost algoritmu během učení neuronové sítě. Binární sigmoidní funkce se používá v sítích, u kterých požadujeme binární výstup, nebo výstup v intervalu 0 až 1. Unipolární sigmoidní funkce si je velmi blízká s binární a používá se u sítí s požadovaným výstupem v rozsahu -1 až 1. [1]</p>

		<div class="image">
			<img src="theory/trfc.png"/>
			<div class="caption">Obr. 2: Sigmoidní aktivační funkce neuronu [3]</div>
		</div>

		
		<h3 id="sec3">Vícevrstvá neuronová síť</h3>
		
		<p>Vícevrstvá neuronová síť (viz obrázek 3) je síť s jednou nebo více skrytými vrstvami neuronů umístěných mezi vstupní a výstupní vrstvou. Neuron vstupní vrstvy posílá svůj vstup ke všem neuronům vnitřní vrstvy. Výstupy vnitřní vrstvy jsou přivedeny na vstupy každého neuronu vyšší vrstvy a jsou vynásobeny příslušnými vahami. Vícevrstvá neuronová síť má potenciál řešit komplikovanější problémy než jednovrstvá, učení takové sítě je ale mnohem složitější. V některých případech je učení vícevrstvé sítě úspěšnější, protože je možné, že se jedná o problém, který nedokáže jednovrstvá síť správně vyřešit. [1]</p>

		<div class="image">
			<img src="theory/network.png"/>
			<div class="caption">Obr. 3: Značení neuronů, vah, vstupů a výstupů v neuronové síti</div>
		</div>

	
		<h3 id="sec4">Volba topologie</h3>
		
		<p>K určení počtu vrstev vícevrstvé neuronové sítě a počtu neuronů v těchto vrstvách neexistuje žádný pevný vztah, topologii neuronové sítě je nutno určit experimentálně. Přesto však existuje několik pravidel a doporučení, která mohou usnadnit volbu její topologie. [2]</p>

		<p>Pro řešení daného problému téměř vždy stačí neuronová síť s jednou vnitřní vrstvou. Topologie se dvěma vnitřními vrstvami může být potřebná, když se neuronová síť má naučit funkci mající nespojitosti. Pro použití více jak dvou vnitřních vrstev neexistuje žádný teoretický důvod. Je zde však možnost, že problém bude efektivněji řešitelný pomocí neuronové sítě s více vrstvami o menším počtu neuronů, než pomocí sítě s menším počtem vrstev s neprakticky velkým počtem neuronů. [2]</p>

		<p>Po zvolení počtu vrstev je třeba zvolit počet neuronů v těchto vrstvách. Volba správného počtu neuronu je velmi důležitá. Při použití malého počtu neuronů nemá neuronová síť kapacitu k naučení daného problému. Naopak při použití příliš velkého počtu neuronů dochází k výraznému prodloužení doby učení a také může nastat problém zvaný přeučení neuronové sítě. [2]</p>

		<p>K přeučení neuronové sítě dochází, když má tato síť příliš mnoho prostředků ke zpracování informací. Je to stav, kdy se neuronová síť příliš přesně naučí množinu trénovacích dat a to včetně jejich náhodných chyb nebo šumu a ztrácí schopnost generalizace. Přeučená neuronová síť dosahuje výborných výsledků s trénovacími daty. Při použití v reálné aplikaci nebo při práci s s testovací množinou dat jsou však výsledky velmi špatné. [2]</p>

		<p>Určit počet neuronů můžeme například tak, že začneme s počtem neuronů, který je příliš malý. Pokud tento počet nelze určit, začneme se dvěma neurony. Dále si určíme způsob výpočtu chyby sítě, což je kritérium posuzující, jak dobře je neuronová síť naučená. Potom postupně zvyšujeme počet neuronů a znovu trénujeme a testujeme síť až do doby, než chyba sítě klesne pod přijatelnou mez, nebo už nedochází k žádnému zlepšení. [2]</p>
		
		
		<h3 id="sec5">Algoritmus backpropagation</h3>

		<p>Učící algoritmus backpropagation je nejpoužívanějším algoritmem v oblasti učení neuronových sítí (přibližně 80% všech aplikací). Algoritmus lze rozdělit do tří hlavních částí, kterými jsou dopředné šíření vstupního signálu, zpětné šíření chyby a aktualizace váhových hodnot vstupů neuronů. V praxi se tyto části cyklicky opakují, dokud není dosaženo dostatečně malé chyby sítě, mezního počtu iterací nebo jiného kritéria pro zastavení procesu učení. [3]</p>

		<p><i>"Během dopředného šíření signálu obdrží každý neuron ve vstupní vrstvě vstupní signál a zprostředkuje jeho přenos ke všem neuronům vyšší (vnitřní) vrstvy. Každý neuron ve vnitřní vrstvě opět vypočítá svůj výstup a pošle ho na vstup další vrstvy. Pokud je další vrstva výstupní, tak je její výstup zároveň výstupem neuronové sítě po předložení vstupního vzoru."</i>	(Skripta Ostravské university [3], strana 36)</p>

		<p>V průběhu učení neuronové sítě metodou backpropagation jsou pro každý trénovací vzor porovnány vypočtené hodnoty výstupů s požadovanými a na základě rozdílu těchto hodnot je definována chyba sítě. Pro tuto chybu je vypočten faktor &delta;<sub>k</sub> (k = 1 ... m), který reprezentuje část chyby šířené z daného neuronu ke všem neuronům nižší vrstvy. Úprava váhových hodnot vstupů neuronů závisí na chybovém faktoru &delta;<sub>k</sub> a hodnotách výstupů neuronů nižší vrstvy.</p>

		<p>K naučení neuronové sítě je kromě patřičného algoritmu učení nutné mít také množinu trénovacích dat. Každý vzor trénovací množiny popisuje požadovaný výstup neuronové sítě při daném vstupu. Za trénovací množinu T můžeme považovat množinu prvků definovaných dvojicemi vektorů vstupu a výstupu: </p>
		
		<div class="image">
			<img src="theory/eq2.png"/>
		</div>
		
		<p>kde T je množina trénovacích dat, S<sub>i</sub> je vektor vstupů neuronové sítě, T<sub>i</sub> je vektor požadovaných výstupů neuronové sítě, q je počet vzorů trénovací množiny, n je počet vstupů neuronové sítě a m je počet výstupů neuronové sítě. [3]</p>
		
		<p>Aktivační funkce neuronu v neuronových sítích používajících algoritmus adaptace vah backpropagation musí splňovat několik požadavků: musí být spojitá, diferencovatelná a monotóně neklesající. [3]</p>

		<p><i>"Cílem adaptace je minimalizace chyby sítě ve váhovém prostoru. Vzhledem k tomu, že chyba sítě přímo závisí na komplikované nelineární složené funkci vícevrstvé sítě, představuje tento cíl netriviální optimalizační problém. Pro jeho řešení se v základním modelu používá nejjednodušší varianta gradientní metody, která vyžaduje diferencovatelnost chybové funkce."</i> (Skripta Ostravské university [3], strana 37)</p>
		
		
		<h3 id="sec6">Literatura</h3>
		<p>[1] FAUSETT, Laurene. <i>Fundamentals of Neural Networks: Architectures, Algorithms And Applications.</i> Prentice-Hall, <br/>&nbsp;&nbsp;&nbsp;&nbsp; 1994. ISBN 978-0-13-334186-7.
		<p>[2] MASTERS, Timothy. <i>Practical Neural Network Recipes in C++.</i> San Francisco: Morgan Kaufmann, 1993. <br/>&nbsp;&nbsp;&nbsp;&nbsp; ISBN 978-0-12-479040-7.</p>
		<p>[3] VOLNÁ, Eva. <i>Neuronové sítě 1.</i> Ostrava, 2002. Učební texty. Ostravská Universita, Přírodovědecká fakulta.</p>
		<div style="float:left; width:768px; height:20px;"></div>
	</div>
</body>
</html> 
 
 
